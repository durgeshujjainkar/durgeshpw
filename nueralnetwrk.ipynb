{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting absl-py (from keras)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\durgesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras) (2.1.1)\n",
      "Collecting rich (from keras)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting h5py (from keras)\n",
      "  Downloading h5py-3.12.1-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting optree (from keras)\n",
      "  Downloading optree-0.14.0-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
      "Collecting ml-dtypes (from keras)\n",
      "  Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\durgesh\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras) (24.1)\n",
      "Collecting typing-extensions>=4.5.0 (from optree->keras)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\durgesh\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 357.1 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 357.1 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 357.1 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 357.1 kB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 357.1 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 0.8/1.3 MB 289.2 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 0.8/1.3 MB 289.2 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 0.8/1.3 MB 289.2 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 0.8/1.3 MB 289.2 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 0.8/1.3 MB 289.2 kB/s eta 0:00:02\n",
      "   -------------------------------- ------- 1.0/1.3 MB 272.0 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.0/1.3 MB 272.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 312.4 kB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading h5py-3.12.1-cp312-cp312-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/3.0 MB 328.9 kB/s eta 0:00:08\n",
      "   ------- -------------------------------- 0.5/3.0 MB 328.9 kB/s eta 0:00:08\n",
      "   ------- -------------------------------- 0.5/3.0 MB 328.9 kB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 0.8/3.0 MB 377.2 kB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 0.8/3.0 MB 377.2 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.0/3.0 MB 406.0 kB/s eta 0:00:05\n",
      "   -------------- ------------------------- 1.0/3.0 MB 406.0 kB/s eta 0:00:05\n",
      "   -------------- ------------------------- 1.0/3.0 MB 406.0 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 1.3/3.0 MB 409.3 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 1.3/3.0 MB 409.3 kB/s eta 0:00:05\n",
      "   --------------------- ------------------ 1.6/3.0 MB 448.6 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 1.6/3.0 MB 448.6 kB/s eta 0:00:04\n",
      "   ------------------------ --------------- 1.8/3.0 MB 481.7 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 2.1/3.0 MB 517.3 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 2.4/3.0 MB 547.8 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 2.4/3.0 MB 547.8 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 563.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 563.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.9/3.0 MB 559.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 562.0 kB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.5.1-cp312-cp312-win_amd64.whl (210 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.0-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, typing-extensions, ml-dtypes, mdurl, h5py, absl-py, optree, markdown-it-py, rich, keras\n",
      "Successfully installed absl-py-2.1.0 h5py-3.12.1 keras-3.8.0 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.0.8 optree-0.14.0 rich-13.9.4 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. How do you create a simple perceptron for basic binary classification?\n",
    "\n",
    "# A perceptron is a basic neural network model used for binary classification. \n",
    "# It consists of an input layer, a single output node, and a simple activation function such as sigmoid.\n",
    "\n",
    "# Hereâ€™s an example in Keras:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a simple perceptron model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input layer with 2 features (input dimension) and a single output unit\n",
    "model.add(Dense(1, input_dim=2, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with data (X_train and y_train are your input and output data)\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. How can you build a neural network with one hidden layer using Keras?\n",
    "\n",
    "# A neural network with one hidden layer is simple and effective for many tasks.\n",
    "# The hidden layer allows the network to learn more complex patterns than a perceptron.\n",
    "\n",
    "# Example using Keras:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a hidden layer with 5 neurons and ReLU activation\n",
    "model.add(Dense(5, input_dim=2, activation='relu'))\n",
    "\n",
    "# Add an output layer with 1 neuron (for binary classification) and sigmoid activation\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. How do you initialize weights using the Xavier (Glorot) initialization method in Keras?\n",
    "\n",
    "# The Xavier (Glorot) initialization method helps avoid issues like vanishing or exploding gradients. \n",
    "# It sets weights based on the number of inputs and outputs in a layer. Keras provides this method with 'glorot_uniform'.\n",
    "\n",
    "# Example using Keras:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "# Create a model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a hidden layer with Xavier initialization\n",
    "model.add(Dense(5, input_dim=2, activation='relu', kernel_initializer=glorot_uniform()))\n",
    "\n",
    "# Add an output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. How can you apply different activation functions in a neural network in Keras?\n",
    "\n",
    "# In Keras, you can use various activation functions like ReLU, Sigmoid, Tanh, etc., by specifying the function in the layer definition.\n",
    "\n",
    "# Example of applying ReLU and Sigmoid:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a hidden layer with ReLU activation\n",
    "model.add(Dense(5, input_dim=2, activation='relu'))\n",
    "\n",
    "# Add an output layer with Sigmoid activation for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. How do you add dropout to a neural network model to prevent overfitting?\n",
    "\n",
    "# Dropout is a technique to prevent overfitting by randomly deactivating some neurons during training.\n",
    "\n",
    "# Example of adding a Dropout layer:\n",
    "\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=2, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout with a 50% dropout rate\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. How do you manually implement forward propagation in a simple neural network?\n",
    "\n",
    "# Forward propagation involves passing inputs through the network, calculating the weighted sum, \n",
    "# and applying the activation function at each layer.\n",
    "\n",
    "# Here's a basic manual implementation of forward propagation:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define input, weights, and bias\n",
    "X = np.array([1, 2])  # Example input\n",
    "weights = np.array([0.5, 0.8])  # Example weights\n",
    "bias = 0.2  # Example bias\n",
    "\n",
    "# Perform the dot product (weighted sum)\n",
    "z = np.dot(X, weights) + bias\n",
    "\n",
    "# Apply activation function (ReLU in this case)\n",
    "activation_output = max(0, z)  # ReLU\n",
    "\n",
    "print(activation_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. How do you add batch normalization to a neural network model in Keras?\n",
    "\n",
    "# Batch normalization helps to stabilize and speed up the training by normalizing the outputs of each layer.\n",
    "\n",
    "# Example of adding batch normalization:\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=2, activation='relu'))\n",
    "model.add(BatchNormalization())  # Adding batch normalization\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. How can you visualize the training process with accuracy and loss curves?\n",
    "\n",
    "# To visualize training progress, we can plot the accuracy and loss curves after training the model.\n",
    "# Keras provides the 'history' object which contains the training metrics.\n",
    "\n",
    "# Example using matplotlib:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=1)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n",
    "\n",
    "# Plot the accuracy curve\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients?\n",
    "\n",
    "# Gradient clipping can prevent gradients from becoming too large, which may cause the training to become unstable.\n",
    "# You can set the gradient clipping by using the 'clipvalue' or 'clipnorm' parameter in the optimizer.\n",
    "\n",
    "# Example using gradient clipping with Adam optimizer:\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Create an Adam optimizer with gradient clipping\n",
    "optimizer = Adam(clipvalue=1.0)  # Clip gradients to a maximum value of 1.0\n",
    "\n",
    "# Compile the model with the optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. How can you create a custom loss function in Keras?\n",
    "\n",
    "# You can create a custom loss function by defining a function that takes true and predicted values\n",
    "# and returns the calculated loss.\n",
    "\n",
    "# Example of a custom mean squared error (MSE) loss function:\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def custom_mse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred))\n",
    "\n",
    "# Compile the model with custom loss function\n",
    "model.compile(loss=custom_mse, optimizer='adam', metrics=['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
