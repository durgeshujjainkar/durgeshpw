{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. How do you perform word tokenization using NLTK and plot a word frequency distribution?\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Sample text\n",
    "text = \"This is a simple text for tokenization and word frequency analysis.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Generate frequency distribution\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "# Plot the frequency distribution\n",
    "fdist.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. How do you use SpaCy for dependency parsing of a sentence?\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy's pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Process the sentence\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Dependency parsing\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. How do you use TextBlob for performing text classification based on polarity?\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Example text\n",
    "text = \"I love programming, it's so much fun!\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Get polarity\n",
    "polarity = blob.sentiment.polarity\n",
    "print(\"Polarity:\", polarity)\n",
    "\n",
    "# Simple classification based on polarity\n",
    "if polarity > 0:\n",
    "    print(\"Positive sentiment\")\n",
    "elif polarity < 0:\n",
    "    print(\"Negative sentiment\")\n",
    "else:\n",
    "    print(\"Neutral sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. How do you extract named entities from a text using SpaCy?\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy's pre-trained model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text\n",
    "text = \"Apple is looking to buy a startup in the UK for $1 billion.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract and print named entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. How can you calculate TF-IDF scores for a given text using Scikit-learn?\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\"\n",
    "]\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Display TF-IDF matrix\n",
    "print(X.toarray())\n",
    "\n",
    "# Display feature names (words corresponding to columns)\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. How do you create a custom text classifier using NLTK's Naive Bayes classifier?\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example dataset: List of tuples with text and label\n",
    "train_data = [\n",
    "    (\"I love programming\", \"positive\"),\n",
    "    (\"This is a great book\", \"positive\"),\n",
    "    (\"I hate this movie\", \"negative\"),\n",
    "    (\"The weather is bad today\", \"negative\")\n",
    "]\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(text):\n",
    "    words = word_tokenize(text)\n",
    "    return {word: True for word in words}\n",
    "\n",
    "# Transform data\n",
    "train_set = [(extract_features(text), label) for text, label in train_data]\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Test the classifier\n",
    "test_sentence = \"I love this book\"\n",
    "print(classifier.classify(extract_features(test_sentence)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. How do you use a pre-trained model from Hugging Face for text classification?\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained model and tokenizer from Hugging Face\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Test the model\n",
    "text = \"I absolutely love this product!\"\n",
    "result = classifier(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. How do you perform text summarization using Hugging Face transformers?\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained summarization model\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Example long text to summarize\n",
    "text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. \n",
    "The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary\n",
    "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. How can you create a simple RNN for text classification using Keras?\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Embedding, GlobalAveragePooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example text data\n",
    "texts = [\"I love programming\", \"This is a great book\", \"I hate this movie\", \"The weather is bad today\"]\n",
    "labels = [1, 1, 0, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X, padding='post')\n",
    "\n",
    "# Build RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=8, input_length=X.shape[1]))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. How do you train a Bidirectional LSTM for text classification?\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example text data\n",
    "texts = [\"I love programming\", \"This is a great book\", \"I hate this movie\", \"The weather is bad today\"]\n",
    "labels = [1, 1, 0, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X, padding='post')\n",
    "\n",
    "# Build Bidirectional LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=8, input_length=X.shape[1]))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. How do you implement GRU (Gated Recurrent Unit) for text classification?\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example text data\n",
    "texts = [\"I love programming\", \"This is a great book\", \"I hate this movie\", \"The weather is bad today\"]\n",
    "labels = [1, 1, 0, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X, padding='post')\n",
    "\n",
    "# Build GRU model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=8, input_length=X.shape[1]))\n",
    "model.add(GRU(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, labels, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. How do you implement a text generation model using LSTM with Keras?\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "# Example text data for text generation (simple)\n",
    "text = \"hello world this is a simple text generation model\"\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
    "\n",
    "# Prepare input-output pairs\n",
    "seq_length = 5\n",
    "sequences = []\n",
    "next_chars = []\n",
    "for i in range(len(text) - seq_length):\n",
    "    sequences.append(text[i:i+seq_length])\n",
    "    next_chars.append(text[i+seq_length])\n",
    "\n",
    "X = np.zeros((len(sequences), seq_length, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, seq in enumerate(sequences):\n",
    "    for t, char in enumerate(seq):\n",
    "        X[i, t, char_to_idx[char]] = 1\n",
    "    y[i, char_to_idx[next_chars[i]]] = 1\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
